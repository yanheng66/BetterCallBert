# build_vector_store.py  â€“  final, tested working
import os, json, faiss, importlib
from glob import glob
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

from llama_index.core import (
    Document,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.core.embeddings.utils import BaseEmbedding    # æ–­è¨€ä½¿ç”¨çš„ç±»

# ---------- å¯è°ƒå‚æ•° ----------
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
JSON_DIR         = "data/parsed_sections_v2"
PERSIST_DIR      = "vector_store_minilm"
BATCH_SIZE       = 64
CHUNK_SIZE       = 512
CHUNK_OVERLAP    = 64
# --------------------------------

st_model = SentenceTransformer(EMBED_MODEL_NAME)

# â‘  è½»é‡é€‚é…å™¨ï¼ˆä»…éœ€ä¸¤æ–¹æ³•ï¼‰
class STAdapter:
    def __init__(self, st, batch_size=32):
        self.st, self.bs = st, batch_size
    def embed_documents(self, texts):
        return (
            self.st.encode(texts, batch_size=self.bs,
                           convert_to_numpy=True).tolist()
        )
    def embed_query(self, text):
        return self.st.encode(text, convert_to_numpy=True).tolist()

adapter = STAdapter(st_model, batch_size=BATCH_SIZE)

# â‘¡ å…³é”®ä¸€æ­¥ï¼šæ³¨å†Œä¸º BaseEmbedding çš„â€œè™šæ‹Ÿå­ç±»â€
BaseEmbedding.register(STAdapter)

splitter = SentenceSplitter(chunk_size=CHUNK_SIZE,
                            chunk_overlap=CHUNK_OVERLAP)

def load_documents():
    docs = []
    for fp in tqdm(glob(f"{JSON_DIR}/*.json"), desc="ğŸ“„ Loading"):
        with open(fp, "r") as f:
            data = json.load(f)
        docs.append(
            Document(
                text=data["text"],
                metadata={
                    "title": data["title"],
                    "section": data["section"],
                    "heading": data.get("heading", ""),
                },
            )
        )
    return docs

def build_vector_store():
    os.makedirs(PERSIST_DIR, exist_ok=True)

    # 1ï¸âƒ£ åˆ†å—
    print("âœ‚ï¸  Chunking â€¦")
    nodes = list(
        tqdm(
            splitter.get_nodes_from_documents(load_documents()),
            desc="Parsing", unit="chunk"
        )
    )
    print("   âœ Chunks generated:", len(nodes))

    # 2ï¸âƒ£ æ‰¹é‡åµŒå…¥
    print("ğŸ”¢ Embedding â€¦")
    texts = [n.get_content() for n in nodes]
    embs  = st_model.encode(
        texts, batch_size=BATCH_SIZE,
        show_progress_bar=True, convert_to_numpy=True
    )
    for n, e in zip(nodes, embs):
        n.embedding = e.tolist()

    # 3ï¸âƒ£ æ„å»º FAISS & ç´¢å¼•
    dim = embs.shape[1]
    vstore = FaissVectorStore(faiss_index=faiss.IndexFlatL2(dim))
    sc     = StorageContext.from_defaults(vector_store=vstore)
    VectorStoreIndex(nodes, storage_context=sc, embed_model=adapter)  # âœ”ï¸ é€šè¿‡æ–­è¨€

    # 4ï¸âƒ£ æŒä¹…åŒ–
    sc.persist(persist_dir=PERSIST_DIR)
    print(f"âœ… å®Œæˆï¼å‘é‡åº“ä¿å­˜äº: {PERSIST_DIR}")

if __name__ == "__main__":
    build_vector_store()
